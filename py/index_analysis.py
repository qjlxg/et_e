# index_analysis.py - ç‹¬ç«‹è·Ÿè¸ªæ ‡çš„é‡åŒ–åˆ†æè„šæœ¬
import akshare as ak
import pandas as pd
import numpy as np
import talib
import re
import time
import random
import sys  # å¼•å…¥sysç”¨äºæ§åˆ¶æ ‡å‡†é”™è¯¯æµè¾“å‡ºå®æ—¶æ—¥å¿—
# å¯¼å…¥ requests å¼‚å¸¸
from requests.exceptions import ConnectionError, Timeout, HTTPError, ChunkedEncodingError, TooManyRedirects
# å¯¼å…¥åº•å±‚ http å®¢æˆ·ç«¯å¼‚å¸¸ï¼Œè§£å†³ RemoteDisconnected é”™è¯¯
import http.client

# --- é…ç½® ---
# è¡¥å……åçš„æŒ‡æ•°åç§°åˆ° AkShare ä»£ç çš„æ˜ å°„
# å·²æ ¹æ®æœ€æ–°éªŒè¯ç»“æœä¿®æ­£éƒ¨åˆ†ä»£ç ï¼Œä»¥æ¶ˆé™¤æ­§ä¹‰å¹¶ç¡®ä¿å‡†ç¡®æ€§ã€‚
INDEX_MAP = {
    'æ²ªæ·±300æŒ‡æ•°': '000300',
    'ä¸­è¯500æŒ‡æ•°': '000905',
    'ä¸­è¯800æŒ‡æ•°': '000906',
    'åˆ›ä¸šæ¿æŒ‡æ•°': '399006',
    'ä¸Šè¯æŒ‡æ•°': '000001',
    'æ’ç”ŸæŒ‡æ•°': 'HSI',
    'ç§‘åˆ›æ¿50æˆä»½æŒ‡æ•°': '000688',
    'ä¸­è¯æ™ºèƒ½æ±½è½¦ä¸»é¢˜æŒ‡æ•°': '399976',
    'ä¸­è¯ç”µå­æŒ‡æ•°': '000807',
    'ä¸­è¯å†›å·¥æŒ‡æ•°': '399967',
    'ä¸­è¯æ–°èƒ½æºæ±½è½¦æŒ‡æ•°': '399808',
    'ä¸­è¯åŒ»è¯å«ç”ŸæŒ‡æ•°': '000933',
    'ä¸­è¯å…‰ä¼äº§ä¸šæŒ‡æ•°': '931151',  # ä¿®æ­£
    'ä¸­è¯äººå·¥æ™ºèƒ½ä¸»é¢˜æŒ‡æ•°': '930713',  # ä¿®æ­£
    'ä¸­è¯ä¼ åª’æŒ‡æ•°': '399971',
    'ä¸­è¯è®¡ç®—æœºä¸»é¢˜æŒ‡æ•°': '930652',  # ä¿®æ­£
    'åˆ›ä¸šæ¿50æŒ‡æ•°': '399673',
    'æ·±åœ³ç§‘æŠ€åˆ›æ–°ä¸»é¢˜æŒ‡æ•°': '399668',
    'ä¸­è¯1000æŒ‡æ•°': '000852',
    'ä¸­è¯ç§‘åˆ›åˆ›ä¸š50æŒ‡æ•°': '931448',
    'ä¸Šè¯ç§‘åˆ›æ¿50æˆä»½æŒ‡æ•°': '000688',
    'ä¸­è¯å…¨æŒ‡ä¿¡æ¯æŠ€æœ¯æŒ‡æ•°': '000993',
    'ä¸­è¯500ä¿¡æ¯æŠ€æœ¯æŒ‡æ•°': '000858',  # ä¿®æ­£
    'ä¸­è¯å…¨æŒ‡åŠå¯¼ä½“äº§å“ä¸è®¾å¤‡æŒ‡æ•°': 'H30184',
    'ä¸­è¯ç§‘æŠ€100æŒ‡æ•°': '931201',
    'ä¸­è¯5Gé€šä¿¡ä¸»é¢˜æŒ‡æ•°': '931079',
    'ä¸­è¯èŠ¯ç‰‡äº§ä¸šæŒ‡æ•°': '930851',  # ä¿®æ­£ï¼ˆä¸»æµç‰ˆï¼‰
    'ä¸­è¯äº‘è®¡ç®—ä¸å¤§æ•°æ®ä¸»é¢˜æŒ‡æ•°': '930651',  # ä¿®æ­£
    'å›½è¯åŠå¯¼ä½“èŠ¯ç‰‡æŒ‡æ•°': '980017',
    'ä¸­è¯æµ·å¤–ä¸­å›½äº’è”ç½‘50äººæ°‘å¸æŒ‡æ•°': 'H30566',
    'ä¸­è¯æ¶ˆè´¹ç”µå­ä¸»é¢˜æŒ‡æ•°': '931098'
}

# MACD å‚æ•°
SHORT_PERIOD = 12
LONG_PERIOD = 26
SIGNAL_PERIOD = 9

# æœ€å¤§é‡è¯•æ¬¡æ•°å’Œè¶…æ—¶è®¾ç½®
MAX_RETRIES = 10  # å¢åŠ åˆ°10æ¬¡ï¼Œæé«˜æˆåŠŸç‡
REQUEST_TIMEOUT = 40  # å»¶é•¿è¶…æ—¶æ—¶é—´

# --- é…ç½®ç»“æŸ ---

def fetch_index_data(index_code, start_date):
    """
    ä½¿ç”¨ AkShare è·å–æŒ‡æ•°çš„æ—¥Kçº¿æ”¶ç›˜ä»·æ•°æ®ï¼Œå¹¶åŠ å…¥å¢å¼ºçš„é‡è¯•æœºåˆ¶ã€‚
    æ‰€æœ‰çš„è­¦å‘Šå’Œé”™è¯¯æ—¥å¿—å°†è¾“å‡ºåˆ° sys.stderrï¼Œå®ç°å®æ—¶ç›‘æ§ã€‚
    """
    for attempt in range(MAX_RETRIES):
        try:
            df = pd.DataFrame()
            if index_code == 'HSI':
                # æ’ç”ŸæŒ‡æ•°
                sys.stderr.write(f" INFO: å°è¯•è·å–æ’ç”ŸæŒ‡æ•° (HSI) æ•°æ®...\n")
                sys.stderr.flush()
                df = ak.index_global_hist(symbol="æ’ç”ŸæŒ‡æ•°", period="daily", start_date=start_date)
            elif index_code.startswith(('H', '93', '98')):  # é’ˆå¯¹ H/93/98 å¼€å¤´çš„ç‰¹æ®ŠæŒ‡æ•°
                sys.stderr.write(f" INFO: å°è¯•è·å–ç‰¹æ®ŠæŒ‡æ•° ({index_code}) æ•°æ®...\n")
                sys.stderr.flush()
                df = ak.index_zh_a_hist(symbol=index_code, period="daily", start_date=start_date)
            else:
                # æ²ªæ·± A è‚¡é€šç”¨æŒ‡æ•° (å¦‚ 000905, 399006)
                sys.stderr.write(f" INFO: å°è¯•è·å– A è‚¡é€šç”¨æŒ‡æ•° ({index_code}) æ•°æ®...\n")
                sys.stderr.flush()
                df = ak.index_zh_a_hist(symbol=index_code, period="daily", start_date=start_date)
           
            # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºå¾ªç¯
            if not df.empty:
                df.rename(columns={'æ—¥æœŸ': 'date', 'æ”¶ç›˜': 'close'}, inplace=True)
                return df[['date', 'close']].set_index('date')
            else:
                # AkShare æ¥å£è¿”å›ç©ºæ•°æ®ï¼Œé€šå¸¸æ„å‘³ç€ä»£ç é”™è¯¯æˆ–æ•°æ®æºæš‚ä¸æ”¯æŒ
                raise ValueError("è·å–æ•°æ®ä¸ºç©ºæˆ– AkShare æ¥å£ä¸æ”¯æŒæ­¤ä»£ç ")
       
        # æ•è·æ‰€æœ‰å¯èƒ½çš„ç½‘ç»œã€è¿æ¥å’Œæ•°æ®é”™è¯¯
        except (ConnectionError, Timeout, http.client.RemoteDisconnected, ValueError, HTTPError, ChunkedEncodingError, TooManyRedirects) as e:
            error_type = e.__class__.__name__
           
            # å®æ—¶æ—¥å¿—è¾“å‡ºåˆ° stderr
            sys.stderr.write(f" è­¦å‘Š: å°è¯• {attempt + 1}/{MAX_RETRIES} - æ— æ³•è·å– {index_code} æ•°æ®: {error_type} - {e}\n")
            sys.stderr.flush()
           
            if attempt < MAX_RETRIES - 1:
                # éšæœºæŒ‡æ•°é€€é¿å»¶è¿Ÿï¼Œé˜²æ­¢è¢«æ•°æ®æºé™æµ
                base_delay = 10  # å¢åŠ åŸºç¡€å»¶è¿Ÿ
                # å¢åŠ éšæœºæ€§å’ŒæŒ‡æ•°å¢é•¿
                sleep_time = random.uniform(base_delay * (attempt + 1), base_delay * (attempt + 2))
                sys.stderr.write(f" ç­‰å¾… {sleep_time:.2f} ç§’åé‡è¯•...\n")
                sys.stderr.flush()
                time.sleep(sleep_time)
            else:
                sys.stderr.write(f" é”™è¯¯: è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({MAX_RETRIES} æ¬¡)ï¼Œæ”¾å¼ƒè·å– {index_code} æ•°æ®ã€‚\n")
                sys.stderr.flush()
                return pd.DataFrame()
       
        except Exception as e:
            sys.stderr.write(f" è‡´å‘½é”™è¯¯: å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œæ— æ³•è·å– {index_code} æ•°æ®: {e.__class__.__name__} - {e}\n")
            sys.stderr.flush()
            return pd.DataFrame()
   
    return pd.DataFrame()

def analyze_and_suggest(df_data, index_name, fund_name):
    """
    å¯¹å•ä¸€æŒ‡æ•°åº”ç”¨ MACD æŒ‡æ ‡ï¼Œå¹¶è¾“å‡ºä¹°å–ä¿¡å·ã€‚
    """
    if len(df_data) < LONG_PERIOD * 2:
        return f" [ {index_name} ] æ•°æ®ä¸è¶³ï¼ˆ{len(df_data)}æ¡ï¼‰ï¼Œè·³è¿‡æŠ€æœ¯åˆ†æã€‚"
   
    # è®¡ç®— MACD æŒ‡æ ‡
    df_nav = df_data.copy()
    # ç¡®ä¿è¾“å…¥æ˜¯ float ç±»å‹ï¼Œå¹¶å¤„ç† NaN
    close_prices = df_nav['close'].fillna(method='ffill').values.astype(float)
   
    df_nav['MACD'], df_nav['MACD_Signal'], df_nav['MACD_Hist'] = \
        talib.MACD(close_prices,
                   fastperiod=SHORT_PERIOD,
                   slowperiod=LONG_PERIOD,
                   signalperiod=SIGNAL_PERIOD)
   
    df_nav['Signal'] = np.where(df_nav['MACD'] > df_nav['MACD_Signal'], 1, 0)
    df_nav['Position'] = df_nav['Signal'].diff()
   
    # æå–æœ€è¿‘çš„äº¤æ˜“ä¿¡å·
    # ç¡®ä¿ä¿¡å·æ—¥æœŸåœ¨å½“å‰æ—¥æœŸä¹‹å‰
    recent_signals = df_nav[df_nav['Position'].abs() == 1].tail(3)
   
    report_output = [f"\n--- ğŸ“ˆ {index_name} ({fund_name} çš„è·Ÿè¸ªæ ‡çš„) æœ€æ–°ä¿¡å· ---"]
   
    if recent_signals.empty:
        report_output.append(" æœªæ£€æµ‹åˆ°æœ‰æ•ˆä¿¡å·ã€‚")
    else:
        for index, row in recent_signals.iterrows():
            action = "ä¹°å…¥/åŠ ä»“ (é‡‘å‰)" if row['Position'] == 1 else "å–å‡º/å‡ä»“ (æ­»å‰)"
            # æ—¥æœŸæ ¼å¼åŒ–ï¼Œå»é™¤æ—¶é—´éƒ¨åˆ†
            date_str = pd.to_datetime(index).strftime('%Y-%m-%d')
            report_output.append(f" æ—¥æœŸ: {date_str}, ä¿¡å·: {action}, æŒ‡æ•°æ”¶ç›˜ä»·: {row['close']:.2f}")
    # åˆ¤æ–­æœ€æ–°çŠ¶æ€
    current_signal = df_nav['Signal'].iloc[-1]
    current_date_str = df_nav.index[-1]
    current_position = "å¤šå¤´ (å»ºè®®æŒæœ‰æˆ–åŠ ä»“)" if current_signal == 1 else "ç©ºå¤´ (å»ºè®®è§‚æœ›æˆ–å‡ä»“)"
    report_output.append(f" å½“å‰çŠ¶æ€ ({current_date_str}): {current_position}")
   
    return "\n".join(report_output)

def main_analysis():
    # 1. è¯»å– fund_basic_data_c_class.csv
    try:
        # ä½¿ç”¨ utf-8-sig åº”å¯¹å¯èƒ½å­˜åœ¨çš„ BOM
        df_funds = pd.read_csv('fund_basic_data_c_class.csv', encoding='utf_8_sig')
    except FileNotFoundError:
        error_msg = "é”™è¯¯ï¼šæœªæ‰¾åˆ° fund_basic_data_c_class.csv æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ‚¨çš„æ•°æ®æŠ“å–å·¥ä½œæµå·²è¿è¡Œã€‚"
        print(error_msg, file=sys.stderr)
        return error_msg
    except Exception as e:
        error_msg = f"è¯»å– CSV æ–‡ä»¶å‡ºé”™: {e}"
        print(error_msg, file=sys.stderr)
        return error_msg
   
    # è®¾ç½®åˆ†ææ•°æ®çš„èµ·å§‹æ—¥æœŸä¸ºä¸€å¹´å‰
    start_date = (pd.Timestamp.today() - pd.DateOffset(years=1)).strftime('%Y%m%d')
    # æŠ¥å‘Šç´¯åŠ å™¨ï¼Œå†…å®¹å°†æœ€ç»ˆè¾“å‡ºåˆ°æ–‡ä»¶
    full_report = [f"ã€åŸºé‡‘è·Ÿè¸ªæ ‡çš„é‡åŒ–åˆ†ææŠ¥å‘Šã€‘\nç”Ÿæˆæ—¶é—´ï¼š{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')} (UTC)\n--------------------------------------------------"]
   
    total_funds = len(df_funds)
   
    # 2. éå†æ¯åªåŸºé‡‘è¿›è¡Œåˆ†æ
    for idx, (index, row) in enumerate(df_funds.iterrows()):
        fund_code = row['åŸºé‡‘ä»£ç ']
        fund_name = row['åŸºé‡‘ç®€ç§°']
        # ç¡®ä¿ tracking_index_str æ˜¯å­—ç¬¦ä¸²ç±»å‹
        tracking_index_str = str(row['è·Ÿè¸ªæ ‡çš„'])
       
        # å®æ—¶è¿›åº¦ä¿¡æ¯è¾“å‡ºåˆ° stderr
        progress_msg = f"[{idx + 1}/{total_funds}] æ­£åœ¨å¤„ç†åŸºé‡‘: {fund_name} ({fund_code}) - è·Ÿè¸ªæ ‡çš„: {tracking_index_str}..."
        sys.stderr.write(progress_msg + '\n')
        sys.stderr.flush()
       
        # 3. æ˜ç¡®è·³è¿‡ 'è¯¥åŸºé‡‘æ— è·Ÿè¸ªæ ‡çš„' æˆ–ä¸ºç©ºçš„è®°å½•
        if tracking_index_str.strip() == 'nan' or tracking_index_str.strip() == 'è¯¥åŸºé‡‘æ— è·Ÿè¸ªæ ‡çš„' or not tracking_index_str.strip():
            full_report.append(f" **è·³è¿‡:** åŸºé‡‘ {fund_name} æ— è·Ÿè¸ªæ ‡çš„ã€‚")
            continue
       
        # æŠ¥å‘Šæ–‡ä»¶å†…å®¹ï¼ˆè¾“å‡ºåˆ° stdoutï¼‰
        header = f"\n==================================================\nğŸ”¬ æ­£åœ¨åˆ†ææŒ‡æ•°åŸºé‡‘: {fund_name} ({fund_code})\n è·Ÿè¸ªæ ‡çš„: {tracking_index_str}\n=================================================="
        full_report.append(header)
       
        # 4. å°è¯•ä»è·Ÿè¸ªæ ‡çš„å­—ç¬¦ä¸²ä¸­åŒ¹é…æŒ‡æ•°åç§° (ä¼˜åŒ–ï¼šå¿½ç•¥å¤§å°å†™ã€æ‹¬å·ã€ç‰¹æ®Šå­—ç¬¦)
        matched_index_name = None
        # ç§»é™¤æ‹¬å·ã€ç©ºæ ¼ã€è¿å­—ç¬¦å¹¶è½¬å°å†™
        cleaned_tracking_str = re.sub(r'[\(\ï¼ˆ\)\ï¼‰\s-]', '', tracking_index_str).strip().lower()
        for name in INDEX_MAP.keys():
            cleaned_name = re.sub(r'[\(\ï¼ˆ\)\ï¼‰\s-]', '', name).strip().lower()
            # ä½¿ç”¨åŒ…å«å…³ç³»è¿›è¡Œå®½æ¾åŒ¹é…
            if cleaned_name in cleaned_tracking_str or cleaned_tracking_str in cleaned_name:
                matched_index_name = name
                break
       
        if not matched_index_name:
            full_report.append(f" **è·³è¿‡:** è·Ÿè¸ªæ ‡çš„ '{tracking_index_str}' æœªåœ¨æ˜ å°„è¡¨ä¸­æˆ–æ— æ³•åŒ¹é…ã€‚")
            continue
       
        index_code = INDEX_MAP[matched_index_name]
        full_report.append(f"\n-> å¼€å§‹åˆ†æè·Ÿè¸ªæ ‡çš„: {matched_index_name} (ä»£ç : {index_code})")
       
        # 5. æŠ“å–æ•°æ®å¹¶åˆ†æ (åŒ…å«é‡è¯•é€»è¾‘)
        df_data = fetch_index_data(index_code, start_date)
       
        if not df_data.empty:
            analysis_result = analyze_and_suggest(df_data, matched_index_name, fund_name)
            full_report.append(analysis_result)
        else:
            full_report.append(f" **é”™è¯¯:** æ— æ³•è·å– {matched_index_name} ({index_code}) çš„å†å²æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æŒ‡æ•°ä»£ç ã€‚")
       
        full_report.append("--------------------------------------------------")
        
        # å…¨å±€å»¶è¿Ÿï¼šæ¯å¤„ç†ä¸€ä¸ªåŸºé‡‘ï¼Œéšæœºç­‰å¾…5-15ç§’ï¼Œå‡å°‘APIè°ƒç”¨é¢‘ç‡
        global_sleep = random.uniform(5, 15)
        sys.stderr.write(f" å…¨å±€å»¶è¿Ÿ: ç­‰å¾… {global_sleep:.2f} ç§’ä»¥é¿å…é™æµ...\n")
        sys.stderr.flush()
        time.sleep(global_sleep)
   
    return "\n".join(full_report)

if __name__ == '__main__':
    # å¿…è¦çš„åº“æ£€æŸ¥
    try:
        import akshare
        # æ£€æŸ¥ talib æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™é€€å‡º
        try:
            import talib
        except ImportError:
            print("è‡´å‘½é”™è¯¯ï¼štalib åº“æœªå®‰è£…æˆ–å®‰è£…å¤±è´¥ã€‚è¯·å…ˆå®‰è£… TA-Lib å¹¶åœ¨ Python ä¸­å®‰è£… talib åº“ã€‚", file=sys.stderr)
            exit(1)
       
        import pandas as pd
        import requests
        import http.client
    except ImportError as e:
        # è‡´å‘½é”™è¯¯è¾“å‡ºåˆ° stderr
        print(f"è‡´å‘½é”™è¯¯ï¼šè¯·ç¡®ä¿å·²å®‰è£… akshare, talib, pandas, requests åº“ã€‚ç¼ºå°‘: {e}", file=sys.stderr)
        exit(1)
   
    report_content = main_analysis()
   
    # æœ€ç»ˆå°†æŠ¥å‘Šå†…å®¹è¾“å‡ºåˆ°æ ‡å‡†è¾“å‡º (ä¼šè¢«é‡å®šå‘åˆ°æ–‡ä»¶)
    print(report_content)
